# LangChain 시작하기

## 1. ChatOpenAI 주요 매개변수와 출력

 - temperature: 사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.
 - model_name: 적용 가능한 모델 리스트
```python
from dotenv import load_dotenv
load_dotenv()

from langchain_openai import ChatOpenAI

# 객체 생성
llm = ChatOpenAI(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    model_name="gpt-4.1-nano",  # 모델명
)

# 질의내용
question = "대한민국의 수도는 어디인가요?"

# 질의
response = llm.invoke(question)
print(response)
print(response.content) # 응답 텍스트 출력
print(response.response_metadata) # 메타데이터 출력
print(response.response_metadata["token_usage"]) # 메타데이터 출력(token_usage 값 출력)
```
<br/>

### 1-1. LogProb 활성화하기

logprob이란 주어진 GPT 모델의 토큰 확률 로그 값을 의미한다. 즉, 모델이 토큰을 예측할 확률을 나타낸다.

logprob은 확률 토큰 값에 자연 로그를 씌워서 변환한 값으로 0에 가까울수록 확률이 높다는 뜻이며, 확률이 낮을수록 음수로 나타난다.

 - bind() 함수를 이용해 logprobs 활성화
    - 각 토큰별로 바이트가 표시되고, 각 토큰의 확률 값이 logprob로 표시된다.
```python
# 객체 생성
llm_with_logprob = ChatOpenAI(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    max_tokens=2048,  # 최대 토큰수
    model_name="gpt-4.1-nano",  # 모델명
).bind(logprobs=True)

# 질의내용
question = "대한민국의 수도는 어디인가요?"

# 질의
response = llm_with_logprob.invoke(question)
```
<br/>

### 1-2. stream() 함수로 스트리밍 출력하기

스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용하다.

 - stream() 함수를 answer 변수에 담고, token.content 단위로 반복문을 실행해 출력해 보면 토큰 단위로 응답 결과가 출력된다.
```python
# 스트리밍 방식으로 각 토큰 출력
answer = llm.stream("대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!")
for token in answer:
    print(token.content, end="", flush=True)

# final_answer 변수에 생성된 문자열 이어붙이기
answer = llm.stream("대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!")
final_answer = ""
for token in answer:
    final_answer += token.contet
```
<br/>

## 2. 멀티모달 모델로 이미지를 인식하여 답변 출력하기

멀티모달 기능은 텍스트뿐만 아니라 이미지, 음성 등 다양한 형태의 데이터를 입력이나 출력으로 처리하는 기술이다.

 - 텍스트: 문서, 책, 웹 페이지 등의 글자로 된 정보
 - 이미지: 사진, 그래픽, 그림 등 시각적 정보
 - 오디오: 음성, 음악, 소리 효과 등의 청각적 정보
 - 비디오: 동영상 클립, 실시간 스트리밍 등 시각적 및 청각적 정보의 결합

```python
from langchain_teddynote.models import MultiModal
from langchain_teddynote.messages import stream_response

# 객체 생성
llm = ChatOpenAI(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    model_name="gpt-4.1-nano",  # 모델명
)

# 멀티모달 객체 생성
multimodal_llm = MultiModal(llm)


# 샘플 이미지 주소(웹사이트로 부터 바로 인식) : 이미지 파일로 부터 질의
IMAGE_URL = "https://t3.ftcdn.net/jpg/03/77/33/96/360_F_377339633_Rtv9I77sSmSNcev8bEcnVxTHrXB4nRJ5.jpg"
answer = multimodal_llm.stream(IMAGE_URL)
stream_response(answer)

# 로컬 PC 에 저장되어 있는 이미지의 경로 입력
IMAGE_PATH_FROM_FILE = "./images/sample-image.png"
answer = multimodal_llm.stream(IMAGE_PATH_FROM_FILE)
stream_response(answer)
```
<br/>

 - `시스템 프롬프트와 사용자 프롬프트 지정하기`
    - 시스템 프롬프트: AI가 특정한 방식으로 반응하도록 지침을 설정하여 응답 스타일이나 목적, 제한 사항을 정의
    - 사용자 프롬프트: AI에게 특정 질문이나 명령을 입력
```python
system_prompt = """당신은 표(재무제표) 를 해석하는 금융 AI 어시스턴트 입니다. 
당신의 임무는 주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는 것입니다."""

user_prompt = """당신에게 주어진 표는 회사의 재무제표 입니다. 흥미로운 사실을 정리하여 답변하세요."""

# 멀티모달 객체 생성
multimodal_llm_with_prompt = MultiModal(
    llm, 
    system_prompt=system_prompt, 
    user_prompt=user_prompt
)

# 로컬 PC 에 저장되어 있는 이미지의 경로 입력
IMAGE_PATH_FROM_FILE = "https://storage.googleapis.com/static.fastcampus.co.kr/prod/uploads/202212/080345-661/kwon-01.png"

# 이미지 파일로 부터 질의(스트림 방식)
answer = multimodal_llm_with_prompt.stream(IMAGE_PATH_FROM_FILE)

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)
stream_response(answer)
```
<br/>

## 3. 프롬프트 템플릿 활용하기

프롬프트란 LLM에게 하는 질문 및 지시 사항과 원하는 출력 답변의 형태를 지정하는 것을 말한다.

프롬프트 템플릿이란 LangChain에서 모델에 제공할 프롬프트의 구조를 미리 정의하고, 사용자가 입력한 변수를 이용해 프롬프트 문자열을 만드는 틀이다. 프롬프트 템플릿을 이용하면 일관된 형식을 유지하면서도 효율적으로 다양한 요청을 처리하고 실수를 줄일 수 있다.

```python
from langchain_core.prompts import PromptTemplate

# template 정의
template = "{country}의 수도는 어디인가요?"

# from_template 함수를 이용하여 PromptTemplate 객체 생성
prompt_template = PromptTemplate.from_template(template)
prompt_template # PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template'{cotunry}의 수도는 어디인가요?')

prompt = prompt_template.format(country="대한민국")
```
<br/>

## 4. LCEL로 체인 생성하기

LCEL은 LangChain Expression Language의 약자로, LangChain에서 프롬프트 → LLM 호출 → 출력 후처리 → 체인 연결 같은 파이프라인을 함수형 스타일로 표현할 수 있게 해주는 도구이다.

즉, 기존에는 LangChain에서 PromptTemplate, LLM, OutputParser 등을 따로따로 붙여야 했는데, LCEL을 사용하면 연산자 체인(|) 형태로 간단하게 연결할 수 있다.

```python
prompt = PromptTemplate.from_template("{topic} 에 대해 쉽게 설명해주세요.")
model = ChatOpenAI(model="gpt-4.1-nano", temperature=0.1)
chain = prompt | model

# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달한다.
input = {"topic": "인공지능 모델의 학습 원리"}
chain.invoke(input)
```
<br/>

## 5. 출력 파서를 체인에 연결하기

출력 파서는 LLM이 생성한 출력 결과를 필요에 맞게 가공하고 구조화하는 단계이다. 출력 텍스트를 분석하여 특정 데이터 혹은 패턴을 추출하거나 사용자의 의도에 맞게 답변을 다듬 역할을 한다.

```python
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = PromptTemplate.from_template("{topic} 에 대해 쉽게 설명해주세요.")
model = ChatOpenAI(model="gpt-4.1-nano", temperature=0.1)
output_parser = StrOutputParser()
chain = prompt | model | output_parser

input = {"topic": "인공지능 모델의 학습 원리"}
chain.invoke(input)
```

 - `예제`
```python
template = """
당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.
양식은 [FORMAT]을 참고하여 작성해 주세요.

#상황:
{question}

#FORMAT:
- 영어 회화:
- 한글 해석:
"""

# 프롬프트 템플릿을 이용하여 프롬프트를 생성합니다.
prompt = PromptTemplate.from_template(template)

# ChatOpenAI 챗모델을 초기화합니다.
model = ChatOpenAI(model_name="gpt-4.1-nano")

# 문자열 출력 파서를 초기화합니다.
output_parser = StrOutputParser()

# 체인을 구성합니다.
chain = prompt | model | output_parser

# 완성된 Chain을 실행하여 답변을 얻습니다.
print(chain.invoke({"question": "저는 식당에 가서 음식을 주문하고 싶어요"}))
```
<br/>

## 6. batch() 함수로 일괄 처리하기

Runnable은 LangChain에서 프롬프트 템플릿, LLM 호출기, 출력 파서 등 다양한 컴포넌트를 연결하고 실행하는 방식을 표준화한 공통 인터페이스이다. 여기서 말하는 컴포넌트란 입력을 구성하는 프롬프트 템플릿, 언어 모델을 호출하는 LLM 래퍼, 모델의 응답을 처리하는 출력 파서 등 체인을 구성하는 기능 단위를 의미한다.

 - stream(): 응답의 청크를 스트리밍
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI()
prompt = PromptTemplate.from_template("{topic} 에 대하여 3문장으로 설명해줘.")
chain = prompt | model | StrOutputParser()

# chain.stream 메서드를 사용하여 '멀티모달' 토픽에 대한 스트림을 생성하고 반복합니다.
for token in chain.stream({"topic": "멀티모달"}):
    # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.
    print(token, end="", flush=True)
```
<br/>

 - invoke(): 입력에 대해 체인을 호출
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI()
prompt = PromptTemplate.from_template("{topic} 에 대하여 3문장으로 설명해줘.")
chain = prompt | model | StrOutputParser()

chain.invoke({"topic": "ChatGPT"})
```
<br/>

 - batch(): 입력 목록에 대해 체인을 호출
    - batch() 함수로 각각 따로 호출해서 답변을 얻는 대신 배치 단위로 묶어 한 번에 처리한다.
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI()
prompt = PromptTemplate.from_template("{topic} 에 대하여 3문장으로 설명해줘.")
chain = prompt | model | StrOutputParser()

# 주어진 토픽 리스트를 batch 처리하는 함수 호출
chain.batch([{"topic": "ChatGPT"}, {"topic": "Instagram"}])

# max_concurrency 매개변수를 사용하여 동시 요청 수를 설정할 수 있다.
chain.batch(
    [
        {"topic": "ChatGPT"},
        {"topic": "Instagram"},
        {"topic": "멀티모달"},
        {"topic": "프로그래밍"},
        {"topic": "머신러닝"},
    ],
    config={"max_concurrency": 3},
)
```
<br/>

## 7. 비동기 호출 방법

 - `async stream: 비동기 스트림`
    - astream() 함수는 비동기 스트림을 생성하여 특정 주제의 메시지를 비동기적으로 처리한다.
    - 이 함수는 비동기 for 반복문(async for)을 활용해 스트림으로부터 메시지를 하나씩 순차적으로 가져온다.
```python
# 비동기 스트림을 사용하여 'YouTube' 토픽의 메시지를 처리합니다.
async for token in chain.astream({"topic": "YouTube"}):
    # 메시지 내용을 출력합니다. 줄바꿈 없이 바로 출력하고 버퍼를 비웁니다.
    print(token, end="", flush=True)
```
<br/>

 - `async invoke: 비동기 호출`
    - ainvoke() 함수는 작업을 비동기적으로 처리하는 함수이다.
    - 이 함수를 호출할 때는 딕셔너리 형태의 인자를 전달한다.
```python
# 비동기 체인 객체의 'ainvoke' 메서드를 호출하여 'NVDA' 토픽을 처리합니다.
my_process = chain.ainvoke({"topic": "NVDA"})

# 비동기로 처리되는 프로세스가 완료될 때까지 기다립니다.
await my_process
```
<br/>

 - `async batch: 비동기 배치`
    - abatch() 함수는 여러 작업을 한꺼번에 비동기적으로 처리하는 일괄 처리 함수이다.
```python
# 주어진 토픽에 대해 비동기적으로 일괄 처리를 수행합니다.
my_abatch_process = chain.abatch(
    [{"topic": "YouTube"}, {"topic": "Instagram"}, {"topic": "Facebook"}]
)

# 비동기로 처리되는 일괄 처리 프로세스가 완료될 때까지 기다립니다.
await my_abatch_process
```
<br/>

## 8. Runnable로 병렬 체인 구성하기

```python
from langchain_core.runnables import RunnableParallel

# {country} 의 수도를 물어보는 체인을 생성합니다.
chain1 = (
    PromptTemplate.from_template("{country} 의 수도는 어디야?")
    | model
    | StrOutputParser()
)

# {country} 의 면적을 물어보는 체인을 생성합니다.
chain2 = (
    PromptTemplate.from_template("{country} 의 면적은 얼마야?")
    | model
    | StrOutputParser()
)

# 위의 2개 체인을 동시에 생성하는 병렬 실행 체인을 생성합니다.
combined = RunnableParallel(capital=chain1, area=chain2)

# 병렬 실행 체인을 실행합니다.
# {'capital': '대한민국의 수도는 서울입니다.', 'area': '대한민국의 면적은 약 100,363.4 제곱 킬로미터 입니다.'}
combined.invoke({"country": "대한민국"})

# 주어진 데이터를 배치로 처리합니다.
# [ {'capital': '대한민국의 수도는 서울이다.', 'area': '대한민국의 면적은 약 100,363km² 입니다.'},
#  {'capital': '미국의 수도는 워싱턴 D.C.입니다.', 'area': '미국의 면적은 약 9,833,520 km² 입니다.'}]
combined.batch([{"country": "대한민국"}, {"country": "미국"}])
```
<br/>

## 9. 데이터를 효과적으로 전달하는 방법

 - RunnablePassthrough는 입력을 변경하지 않거나 추가 키를 더하여 전달할 수 있다.
 - RunnablePassthrough가 단독으로 호출되면, 단순히 입력을 받아 그대로 전달한다.
 - RunnablePassthrough.assign() 방식으로 호출되면, 입력을 받아 assign 함수에 전달된 추가 인수를 추가한다.

```python
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# Chain 생성
prompt = PromptTemplate.from_template("{num} 의 10배는?")
llm = ChatOpenAI(temperature=0)
chain = prompt | llm

# 1. Chain 실행
chain.invoke({"num": 5})

# 2. RunnablePassthrough 이용
# RunnablePassthrough 는 runnable 객체이며, runnable 객체는 invoke() 메소드를 사용하여 별도 실행이 가능하다.
from langchain_core.runnables import RunnablePassthrough
RunnablePassthrough().invoke({"num": 10})

# 3. RunnablePassthrough 로 체인 구성
runnable_chain = {"num": RunnablePassthrough()} | prompt | ChatOpenAI()
runnable_chain.invoke(10)

# 4. RunnablePassthrough.assign(): 입력 값으로 들어온 값의 key/value 쌍과 새롭게 할당된 key/value 쌍을 합친다.
# {'num': 1, 'new_num': 3}
(RunnablePassthrough.assign(new_num=lambda x: x["num"] * 3)).invoke({"num": 1})
```
<br/>

## 10. RunnableParallel

RunnableParallel은 여러 Runnable 객체를 병렬로 실행해준다. 키-값 구조로 여러 개의 Runnable을 설정하고, 최종적으로 각 키 값을 결과로 반환한다.

```python
from langchain_core.runnables import RunnableParallel

# RunnableParallel 인스턴스를 생성합니다. 이 인스턴스는 여러 Runnable 인스턴스를 병렬로 실행할 수 있습니다.
runnable = RunnableParallel(
    # RunnablePassthrough 인스턴스를 'passed' 키워드 인자로 전달합니다. 이는 입력된 데이터를 그대로 통과시키는 역할을 합니다.
    passed=RunnablePassthrough(),

    # 'extra' 키워드 인자로 RunnablePassthrough.assign을 사용하여, 'mult' 람다 함수를 할당합니다. 이 함수는 입력된 딕셔너리의 'num' 키에 해당하는 값을 3배로 증가시킵니다.
    extra=RunnablePassthrough.assign(mult=lambda x: x["num"] * 3),

    # 'modified' 키워드 인자로 람다 함수를 전달합니다. 이 함수는 입력된 딕셔너리의 'num' 키에 해당하는 값에 1을 더합니다.
    modified=lambda x: x["num"] + 1,
)

# runnable 인스턴스에 {'num': 1} 딕셔너리를 입력으로 전달하여 invoke 메소드를 호출합니다.
runnable.invoke({"num": 1})

# {'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}
```
<br/>

## 11. RunnableLambda

RunnableLambda는 사용자 정의 함수를 매핑하여 프롬프트를 입력하기 전에 실행하고, 그 결과를 프롬프트에 전달하는 Runnable 객체이다.

이를 통해 파이썬으로 구현한 로직이나 API 호출 등을 함수로 감싸서 먼저 실행한 후 그 결과값을 프롬프트 입력으로 활용할 수 있다.

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from datetime import datetime
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

def get_today(a):
    # 오늘 날짜를 가져오기
    return datetime.today().strftime("%b-%d")

# Chain 생성
prompt = PromptTemplate.from_template(
    "{today} 가 생일인 유명인 {n} 명을 나열하세요. 생년월일을 표기해 주세요."
)
llm = ChatOpenAI(temperature=0, model_name="gpt-4o")
chain = (
    {"today": RunnableLambda(get_today), "n": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 출력
print(chain.invoke(3))
```
<br/>

 - `itemgetter`
    - itemgetter는 딕셔너리에서 특정 키에 해당하는 값을 추출한다.
```python
from operator import itemgetter

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI


# 문장의 길이를 반환하는 함수입니다.
def length_function(text):
    return len(text)


# 두 문장의 길이를 곱한 값을 반환하는 함수입니다.
def _multiple_length_function(text1, text2):
    return len(text1) * len(text2)


# _multiple_length_function 함수를 사용하여 두 문장의 길이를 곱한 값을 반환하는 함수입니다.
def multiple_length_function(_dict):
    return _multiple_length_function(_dict["text1"], _dict["text2"])


prompt = ChatPromptTemplate.from_template("{a} + {b} 는 무엇인가요?")
model = ChatOpenAI()
chain1 = prompt | model

chain = (
    {
        "a": itemgetter("word1") | RunnableLambda(length_function),
        "b": {"text1": itemgetter("word1"), "text2": itemgetter("word2")}
        | RunnableLambda(multiple_length_function),
    }
    | prompt
    | model
)

chain.invoke({"word1": "hello", "word2": "world"})

# AIMessage(content='5 + 25 는 30입니다.')
```
