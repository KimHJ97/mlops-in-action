# 랭체인

 - 랭체인을 위한 준비사항
 - 코랩 환경
 - LLM API 키
 - 허깅페이스
 - 랭체인 구성요소


## 1. 랭체인을 위한 준비사항

랭체인은 언어 모델이 단순한 질의 응답을 넘어서 더욱 복잡한 워크플로나 애플리케이션을 지원할 수 있게 도와주는 프레임워크이다. 이를 통해 개발자는 언어 모델에 다양한 외부 데이터를 결합하거나 대규모 데이터를 처리하는 복잡한 프로세스를 관리할 수 있다.

### 1-1. 체인을 사용하는 이유

랭체인은 자연어 처리 모델을 더 효율적으로 사용하기 위해 개발된 프레임워크이다. 언어 모델을 단순한 텍스트 생성 도구로 사용하는 것을 넘어, 외부 데이터 소스(DB, API, 문서) 와 결합해 사용하고 싶을 때 유용하다.

 - 언어 모델은 단순한 질문에 대한 답 뿐만 아니라, 필요한 경우 외부 데이터에서 실시간 정보를 검색한 후 그 데이터를 바탕으로 보다 정확하고 유용한 응답을 생성할 수 있다.
    - 대형 언어 모델의 한계를 보완하고 최신 정보를 반영한 응답을 제공한다.

### 1-2. 랭체인의 핵심 구성 요소

 - __LLM 인터페이스 (언어 모델 API)__
    - 랭체인은 대규모 언어 모델과 상호작용할 수 있는 API를 제공한다.
    - 개발자는 ChatGPT, Gemini, PaLM과 같은 언어 모델을 랭체인을 통해 간단한 API 호출로 연결하고 쿼리할 수 있다.
 - __프롬프트 템플릿 (일관된 질의, 재사용 가능한 템플릿, 작업 최적화)__
    - 프롬프트 템플릿은 언어 모델에 입력될 명령어 또는 쿼리를 구조화하는 미리 정의된 툴이다.
    - 이를 통해 일관성 있는 질의 응답 처리가 가능하며, 재사용이 가능하다.
 - __에이전트 (작업 흐름 관리, 외부 상호작용, 작업 순서 지정)__
    - 에이전트는 복잡한 작업을 자동화할 수 있는 구성 요소로 여러 단계에 걸쳐 실행될 작업 흐름를 관리한다.
    - 에이전트는 언어 모델이 주어진 쿼리에 대해 최적의 작업 순서를 결정할 수 있게 하며, 외부 데이터 소스나 도구들과 상호작용하여 원하는 결과를 얻을 수 있다.
 - __검색 모듈 (RAG 도구, 벡터 데이터베이스, 의미적 검색)__
    - 랭체인은 검색 증강 생성(RAG)을 위한 다양한 도구를 제공하여 언어 모델이 새로운 데이터에 접근하고 이를 기반으로 응답을 생성할 수 있게 해준다.
    - 특히, 벡터 데이터베이스와 결합하여 정보의 의미적 표현을 생성하고 이를 효율적으로 검색하는 기능을 지원한다.
 - __메모리 (맥락 유지, 대화 분석, 지속적인 상호작용)__
    - 메모리 기능을 통해 이전 대화나 상호작용의 맥락을 기억하고, 이를 기반으로 다음 응답에 활용할 수 있다.
 - __콜백 (이벤트 모니터링, 오류 추적, 성능 분석)__
    - 콜백은 랭체인 작업 중 발생하는 이벤트를 추적하고 모니터링하는 데 사용된다.
    - 체인의 호출 시점, 오류 발생 시점 등을 기록하고, 이를 실시간으로 스트리밍하거나 분석할 수 있다.

## 3. LLM API 키

ChatGPT나 Gemini 같은 최신 대형 언어 모델을 활용하기 위해서는 각 플랫폼에서 제공하는 API 키를 발급받아야 한다.

 - `ChatGPT 주요 기능`
    - LLM: 대규모 데이터를 학습한 언어 모델로, 텍스트 생성, 번역, 요약, 질의 응답 등 다양한 작업을 수행할 수 있다.
    - 임베딩 모델: 텍스트를 고차원 벡터로 변환하는 모델이다. 이를 통해 유사한 의미를 가진 텍스트를 수치적으로 표현할 수 있으며, 검색 엔진 최적화, 문서 분류, 추천 시스템 등에 활용된다. 임베딩 모델은 언어 간 비교나 의미 유사도 측정 등 다양한 응용 분야에서 매우 유용하다.
    - 파인튜닝: OpenAI의 기본 모델을 사용자가 원하는 특정 작업이나 데이터에 맞게 추가 학습시켜 최적화하는 방법이다. 이를 통해 더 정교한 응답을 생성하거나 특정 도메인에 맞는 언어 모델을 만들 수 있다.

```python
# OpenAI ChatGPT 사용 예시
from openai import OpenAI
import os

os.environ['OPENAI_API_KEY'] = 'API 키 입력'

client = OpenAI()
completation = client.chat.completions.create(
    model = "gpt-4o-mini",
    messages = [
        {"role": "system", "content": "너는 상담원이야"},
        {"role": "user", "content": "서울 명소는?"}
    ]
)

print(completion.choices[0].message.content)

# Google Gemini 사용 예시
import google.generativeai as genai
import os

genai.configure(api_key = "API 키 입력")

model = genai.GenerativeModel("gemini-1.5-flash")
response = model.generate_content("서울 명소는?")

print(response.text)
```

## 4. 허깅페이스

허깅페이스는 자연어 처리 및 인공지능 모델을 쉽게 사용하고 개발할 수 있도록 도와주는 플랫폼이다. 이 회사는 주로 트랜스포머 모델과 같은 대규모 언어 모델을 오픈소스로 제공하여 개발자와 연구자들이 다양한 애플리케이션에 활용할 수 있게 지원한다.

 - Transformers 라이브러리: BERT, GPT, T5 등과 같은 대규모 언어 모델들을 쉽게 가져다 사용할 수 있다. 이 라이브러리는 연구자나 개발자가 복잡한 모델을 구축하는 과정에서 많은 시간을 절약할 수 있게 돕는다.
 - 모델 허브: 허깅페이스는 방대한 모델 허브를 운영하고 있으며, 이 허브에는 수천 개의 사전 학습된 모델이 저장되어 있다.
 - 데이터셋 허브: 허깅페이스는 다양한 언어 및 작업에 맞춘 데이터셋도 제공한다.
 - Inference API: 별도의 코드 작성 없이도 API를 통해 모델을 사용할 수 있다.
 - Spaces: 사용자들이 직접 만든 모델을 웹 인터페이스로 시연할 수 있는 플랫폼으로, Gradio 기반의 데모 애플리케이션을 쉽게 만들고 공유할 수 있다.

## 5. 랭체인 구성요소

랭체인은 언어 모델을 기반으로 애플리케이션을 개발하기 위한 오픈소스 프레임워크이다. 사용자 쿼리에 대한 응답을 생성하고, 문서나 구조화된 데이터를 요약하거나 분석하고, 정보를 맞춤으로 가져와 정확성을 개선하는 데 활용할 수 있다. 또한 코드 작성과 이해, API와의 상호작용, 생성형 AI를 활용한 다양한 애플리케이션 개발이 가능하다.

### 5-1. 체인

랭체인에서 체인은 여러 언어 모델을 연결해 일관된 워크플로를 구성하는 핵심적인 요소이다. __체인은 개별 작업 단계를 순서대로 이어 붙여 하나의 흐름을 만든다.__

체인은 작업의 결과를 단계별로 연결하며 각 단계에서 얻은 결과를 다음 단계로 전달하는 방식으로 작동한다. 먼저, 사용자가 제공한 텍스트를 분석하여 질문을 이해하고 필요한 작업을 결정하는 입력 텍스트 처리 과정을 거친다. 이후, 필요에 따라 검색, 데이터베이스 조회, API 호출 등 외부 데이터 소스에 접근하여 실시간 정보나 특정 데이터를 가져온다. 마지막으로, 이렇게 얻은 데이터를 바탕으로 최종 응답을 생성하며, 이를 통해 더 정확하고 맥락에 맞는 답변을 제공한다.

 - `체인 실행 예시`
    - 생성된 모델은 invoke 함수를 통해 답변을 생성할 수 있다.
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model = "gpt-4o-mini")
llm.invoke("피타고라스 정리의 공식은 무엇인가요?")
```

 - `문자열 형태 템플릿 프롬프트`
    - ChatPromptTemplate.from_template() 를 통해서 문자열 형태의 템플릿을 받아 프롬프트를 입력할 수 있다.
```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("초등학생이 이해할 수 있는 방식으로 답을 설명하세요.: <질문>: {query}")

llm = ChatOpenAI(model = "gpt-4o-mini")

output_parser = StrOutputParser()

# LCEL chaining
chain = prompt | llm | output_parser

# chain 호출
chain.invoke({"query": "피타고라스 정리의 공식은 무엇인가요?"})
```

 - `여러 체인을 연결하고 병렬 처리`
   - RunnableParallel을 사용하여 병렬로 처리함으로써 각 체인의 실행을 최적화하고 보다 빠르게 결과를 도출할 수 있다.
   - ChatOpenAI: OpenAI의 GPT 모델을 사용하여 대화를 생성하거나 질문에 답하는 데 필요한 기능 제공
   - ChatPromptTemplate: 텍스트 기반의 프롬프트를 동적으로 생성하기 위한 템플릿 정의
   - StrOutputParser: 모델의 응답을 문자열로 변환하고 파싱하는 데 사용
   - RunnablePassthrough: 데이터를 변환 없이 그대로 통과시켜주는 역할을 하는 래퍼
   - RunnableParallel: 여러 작업을 병렬로 실행할 수 있게 해주는 기능
   - itemgetter: 파이프라인에서 데이터의 특정 부분을 추출하는 데 사용
```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableMap
from operator import itemgetter

# 1. 주제에 대한 기본 논의 생성
basictopic = (
   # 입력된 주제에 대한 논의 생성을 위한 프롬프트 설정 "{topic}"은 사용자로부터 받은 주제 값을 동적으로 설정
   ChatPromptTemplate.from_template("{topic}에 대한 논쟁을 한국어로 생성합니다.")

   # OpenAI의 GPT 모델 호출하여 논의 내용 생성
   | ChatOpenAI

   # 생성된 논의 내용을 문자열 형식으로 파싱
   | StrOutputParser()

   # 파싱된 결과를 그대로 "base"라는 이름으로 넘김
   | {"base": RunnablePassthrough()}
)

# 2. 긍정적인 의견 생성
# base에서 생성된 논의 결과를 기반으로 긍정적인 측면을 나열하는 체인 정의
positive = (
   # base라는 입력을 받아 프롬프트 설정
   ChatPromptTemplate.from_template("{base}의 장점 또는 긍정적인 측면을 나열하세요.")

   # OpenAI의 GTP 모델 호출하여 긍정적인 측면을 나열하는 작업 수행
   | ChatOpenAI()

   # 생성된 내용을 문자열로 파싱
   | StrOutputParser()
)

# 3. 부정적인 의견 생성
negative = (
   # base라는 입력을 받아 프롬프트 설정
   ChatPromptTemplate.from_template("{base}의 단점 또는 부정적인 측면을 나열하세요.")

   # OpenAI의 GTP 모델 호출하여 부정적인 측면을 나열하는 작업 수행
   | ChatOpenAI()

   # 생성된 내용을 문자열로 파싱
   | StrOutputParser()
)

# 4. 최종 답변 생성
# positive와 negative에서 생성된 결과를 바탕으로 최종 답변을 생성하는 체인인 final 정의
final = (
   # 이전에 생성된 긍정적 의견(results_1)과 부정적 의견(results_2)을 통합하여 최종 비평을 생성하는 프롬프트 설정
   ChatPromptTemplate.from_messages(
      [
         ("ai", "{original_response}"),
         ("human", "긍정:\n{results_1}\n\n부정:\n{results_2}"),
         ("system", "비평에 대한 최종 답변 생성")
      ]
   )

   # OpenAI의 GTP 모델 호출하여 최종 비평 생성
   | ChatOpenAI()

   # 생성된 내용을 문자열로 파싱
   | StrOutputParser()
)

# 5. 병렬 처리
# RunnableParallel을 사용하여 positive와 negative를 병렬로 처리
chain = (
   basictopic
   | RunnableParallel(
      results_1 = positive,
      results_2 = negative,
      original_response = itemgetter("base")
   )
   | RunnableMap(
      {
         "positive_result": itemgetter("results_1"),
         "negative_result": itemgetter("results_2"),
         "final_answer": final,
      }
   )
)

# 6. 최종 실행
result = chain.invoke({"topic": "social media"})

positive_result = result['positive_result']
negative_result = result['negative_result']
final_answer = result['final_answer']

print("소셜미디어에 대한 긍정 의견:\n", positive_result)
print("\n소셜미디어에 대한 부정 의견:\n", negative_result)
print("\n최종 의견", final_answer)
```
<br/>

### 5-2. 프롬프트

프롬프트는 모델에게 주어지는 입력으로, 특정한 작업을 수행하도록 유도하는 질문이나 명령어이다. 프롬프트 템플릿은 개발자가 언어 모델과 상호작용하는 방식을 구조화하여 일관성 있는 응답을 얻도록 도와주는 사전 구축된 구조이다.

프롬프트 템플릿을 활용하면 동일한 질문에 대한 일정한 품질과 형식 답변을 제공하여 일관성 있는 응답을 보장한다. 또한, 자주 사용하는 질문이나 작업을 반복적으로 활용하여 재사용 가능성이 높다.

 - `PromptTemplate 기본 사용법`
```python
from langchain_core.prompts import PromptTemplate

# 템플릿 생성
prompt_template = PromptTemplate.from_template(
   "{content}에 대한 {adjective} 농담을 들려주세요."
)

# 템플릿 값 설정
formatted_prompt = prompt_template.format(adjective="재미있는", content="닭")
print(formatted_prompt)
```

 - `채팅 프롬프트`
   - ChatPromptTemplate은 대화형 모델, 즉 채팅 모델을 위한 프롬프트를 생성하는 템플릿이다.
   - 메시지의 역할을 지정할 수 있으며, 각 메시지는 system, human, ai로 구분된다.
```python
from langchain_core.prompts import ChatPromptTemplate

# 대화형 템플릿 생성
chat_template = ChatPromptTemplate.from_message(
   [
      ("system", "너는 유능한 어시스트야. 너의 이름은 {name} 이야."),
      ("human", "안녕하세요?"),
      ("ai", "안녕하세요 무엇을 도와드릴까요?"),
      ("human", "{user_input}"),
   ]
)

# 템플릿 값 설정
messages = chat_template.format_messages(name="김철수", user_input="너의 이름은 뭐야?")
print(messages)
```

 - `프롬프트 조합 및 변수 사용하기`
   - MessagesPlaceholder는 프롬프트 내에서 동적으로 메시지 리스트를 삽입할 수 있다.
   - ChatPromptTemplate: 여러 메시지로 구성된 프롬프트를 만들기 위한 템플릿
   - HumanMessagePromptTemplate: 사람이 입력하는 메시지를 템플릿화
   - MessagesPlaceholder: 과거 대화 기록 등을 전달할 때 사용
   - HumanMessage, AIMessage: 실제 메시지 형식을 표현하는 객체
```python
from langchain_core.prompts import (
   ChatPromptTemplate,
   HumanMessagePromptTemplate,
   MessagesPlaceholder,
)

from langchain_core.messages import HumanMessage, AIMessage

# 대화 요약 템플릿 정의
human_prompt = "지금까지의 대화를 {word_count} 단어로 요약합니다."
human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)

# 대화 내용과 함께 요약 요청
# 두 가지 메시지로 구성된 프롬프트 템플릿을 생성한다.
chat_prompt = ChatPromptTemplate.from_messages(
   [MessagesPlaceholder(variable_name="conversation"), human_message_template]
)

# 대화 내용과 단어 수를 넣어서 요약 프롬프트 생성
conversation = [
   HumanMessage(content="프로그래밍을 배우는 가장 좋은 방법은 무엇인가요?"),
   AIMessage(content="변수 및 루프와 같은 기본 사항부터 시작하세요.")
]
formatted_prompt = chat_prompt.format_prompt(conversation=conversation, word_count="10")

# .to_messages()는 LangChain에서 사용하는 Message 객체 리스트를 반환
# 이 메시지 리스트는 실제 LLM 호출 시 사용된다.
# [
#  HumanMessage(content="프로그래밍을 배우는 가장 좋은 방법은 무엇인가요?"),
#  AIMessage(content="변수 및 루프와 같은 기본 사항부터 시작하세요."),
#  HumanMessage(content="지금까지의 대화를 10 단어로 요약합니다.")
# ]
print(formatted_prompt.to_messages())
```

### 5-3. 메모리

메모리는 랭체인에서 이전 대화나 상호작용의 맥락을 유지하여 연속적인 대화를 가능하게 하며 과거 대화 정보를 활용하여 더 자연스럽고 관련성 높은 답변을 제공하는 데 중요한 역할을 한다.

ReAct 에이전트에 메모리 기능을 추가하여 대화의 연속성을 확보할 수 있다. 에이전트가 대화의 맥락을 기억하고, 보다 유용하고 일관된 대화를 할 수 있게 된다.

 - `사용자 프로필 관리`
   - 메모리가 없으면 에이전트는 사용자가 이전에 자신을 소개했던 내용을 기억하지 못하여 적절한 응답을 제공하지 못한다.
   - 메모리 기능을 추가하기 위해 ChatMessageHistory와 RunnableWithMessageHistory 클래스를 활용하여 대화의 기록을 저장하고, 이를 통해 에이전트가 이전 대화 내용을 기억하도록 구현할 수 있다.
 - `1. 메모리 클래스 정의`
   - pydantic.BaseModel을 상속하여 입력 데이터 유효성 검사 클래스 생성
   - Field(description="..."): 각 필드에 대한 설명 추가 (LangChain에서 tool 설명에 활용됨)
   - @validator: 각 필드 값에 대해 커스텀 유효성 검사 수행
```python
# 1. 메모리 클래스 정의
from pydantic import BaseModel, Field, validator
from langchain_core.tools import StructuredTool
from langchain_core.tools import ToolException

class UserProfileInput(BaseModel):
   name: str = Field(description="사용자 이름")
   language: str = Field(description="사용자 언어")

   @validator('name')
   def validate_name(cls, v):
      if not v or len(v) < 1:
         raise ToolException('Name cannot be empty')
      return v
   
   @validator('language')
   def validate_language(cls, v):
      if not v or len(v) < 1:
         raise ToolException('Language cannot be empty')
      return v
```

 - `도구 정의`
   - ✅ update_user_profile(name, language)
      - 사용자 이름과 언어를 받아 프로필을 업데이트하는 함수.
      - 실제로는 DB 연동이 들어가야 함 (지금은 문자열만 반환).
   - ✅ get_user_profile()
      - 저장된 사용자 프로필을 반환.
   - ✅ StructuredTool.from_function(...)
      - 일반 Python 함수를 LangChain에서 사용할 수 있는 도구(Tool)로 변환.
      - args_schema로 입력 구조를 지정 (Pydantic 기반).
      - handle_tool_error=True: 오류 발생 시 LangChain 에이전트가 처리할 수 있도록 설정.
```python
# 2. 도구 정의
def update_user_profile(name: str, language: str) -> str:
   """이름과 선호하는 언어로 사용자 프로필 업데이트"""
   # 실제로는 DB 연동
   return f"프로필 업데이트 완료: name: {name}, language: {language}"

def get_user_profile() -> str:
   """사용자 프로필 정보 검색"""
   # 실제로는 DB 연동
   return "사용자 프로필 가져오기: name: Alex, language: 프랑스어"

update_profile_tool = StructuredTool.from_function(
   func=update_user_profile,
   args_schema=UserProfileInput,
   handle_tool_error=True,
)

get_profile_tool = StructuredTool.from_function(
   func=get_user_profile,
   handle_tool_error=True,
)
```

 - `3. 메모리 기반 에이전트 설정`
```python
# 3. 메모리 기반 에이전트 설정
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.agents import create_tool_calling_agent
from langchain_core.agents import AgentExecutor

# 시스템, 히스토리, 입력, scratchpad 구성의 프롬프트 템플릿 생성.
# LangChain agent가 어떤 도구를 쓸지 결정하기 위한 프롬프트 구조입니다.
prompt = ChatPromptTemplate.from_message([
   ("system", "You're a helpful assistant"),
   ("placeholder", "{history}"),
   ("human", "{input}"),
   ("placeholder", "{agent_scratchpad}"),
])

tools = [update_profile_tool, get_profile_tool]

# 도구를 사용할 수 있는 LangChain agent 생성.
# llm은 사전에 정의된 LLM 인스턴스 (예: OpenAI, Anthropic 등)이어야 함.
agent = create_tool_calling_agent(llm, tools, prompt)

# agent와 tool을 연결해서 실행 환경을 구성.
# verbose=True는 디버깅 출력을 활성화.
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

store = {} # 메시지 기록을 저장하는 더미 데이터베이스

def get_session_history(session_id: str) -> BaseChatMessageHistory:
   if session_id not in store:
      store[session_id] = ChatMessageHistory()
   return store[session_id]

# RunnableWithMessageHistory: LangChain의 메모리 연동 기능을 제공.
# 에이전트 실행 시 이전 대화 내용(history)을 자동으로 주입해줌.
agent_executor_w_memory = RunnableWithMessageHistory(
   agent_executor,
   get_session_history,
   input_messages_key="input",
   history_messages_key="history"
)
```

 - `사용 예시`
   - 메모리 기능을 추가함으로써 ReAct 에이전트는 대화의 맥락을 기억하고 더 연속적이며 일관된 응답을 제공할 수 있다.
```python
agent_executor_w_memory.invoke(
   {"input": "안녕하세요, 저는 김철수입니다."},
   config = {
      "configurable": {
         "session_id": "user1234"
      }
   },
)

agent_executor_w_memory.invoke(
   {"input": "내가 누구인지 기억나?"},
   config = {
      "configurable": {
         "session_id": "user1234"
      }
   },
)
```

### 5-4. 인덱스

특정 작업을 수행하기 위해서는 데이터 내에 포함되지 않은 파일, 즉 외부 데이터를 처리할 수 있어야 한다. 인덱스는 대규모 데이터셋에서 필요한 정보를 빠르게 찾아내는 데 매우 중요한 역할을 하며, 특히 검색 증강 생성(RAG) 기반 애플리케이션에서 필수적인 요소로 사용된다.

 - `DocumentLoader`
   - DocumentLoader는 다양한 형식의 문서를 효과적으로 로드하고 처리할 수 있는 도구이다.
   - 문서를 벡터 데이터베이스에 추가하기 전에 불필요한 정보나 중복된 내용을 걸러낼 수 있어 데이터 품질을 향상시킬 수 있다. 대량의 문서를 다룰 떄 유용하며, 다양한 기능을 통해 작업 과정을 단순화하고 효율성을 높여준다.
   - PDF, CSV, 텍스트, 웹 페이지 같은 다양한 형식 파일 지원
```python
# TextLoader 활용
from langchain_community.document_loaders import TextLoader

loader = TextLoader("./README.md")
document = loader.load()
print(document)

# 웹페이지 URL 활용
from langchain_community.document_loaders import WebBaseLoader

url = "https://n.news.naver.com/mnews/article/055/001207714"
loader = WebBaseLoader(url)
document = loader.load()
print(document)

# PDF 문서 활용
from langchain.document_loaders import PyPDFLoader

pdf_path = "./test.pdf"
loader = PyPDFLoader(pdf_path)
document = loader.load()
print(document)
```

 - `Text Splitter`
   - Text Splitter는 긴 문서나 문단을 작은 청크 단위로 분할하여 검색 엔진이 효율적으로 인덱싱하고 검색할 수 있도록 돕는 도구이다.
   - 이 기능은 특히 NLP 모델이나 벡터 데이터베이스가 요구하는 입력 길이에 맞춰 문서를 처리하는 데 유용하다. 긴 텍스트를 적절한 크기로 나눔으로써 인덱싱 과정에서 정보 손실을 방지하고, 문서를 효율적으로 관리할 수 있게 한다.
```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

loader = TextLoader("./README.md")
splitter = RecursiveCharacterTextSplitter(chunk_size=500) # 500자 단위로 분할

# 문서를 로드하면서 동시에 분할
documents = loader.load_and_split(text_splitter=splitter)

# 각 청크 확인
for doc in documents:
   print(doc)
```

### 5-5. 벡터 데이터베이스

벡터 데이터베이스는 고차원 벡터로 변환된 텍스트 데이터를 저장하고, 이를 기반으로 유사성 검색을 가능하게 하는 데이터베이스이다. 대규모 데이터셋에서 유의미한 정보를 빠르게 추출할 수 있게 해주며 검색 증강 생성(RAG)에서 중요한 요소로 작용한다.

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader

# 텍스트 문서 로드
loader = TextLoader("./README.md")
documents = loader.load()

# 문서 내용을 리스트로 변환
texts = [doc.page_content for doc in documents]
print(texts)

# OpenAI 임베딩 모델로 텍스트 임베딩 생성
# OpenAI에서 만든 "text-embedding-ada-002" 임베딩 모델로 texts의 각 문장을 숫자 벡터로 변환
embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
document_embeddings = embedding_model.embed_documents(texts)

# FAISS 벡터 데이터베이스 생성 및 임베딩 저장
# texts를 하나씩 순회하면서, 내부에서 자동으로 embedding_model.embed_documents(texts)를 호출하여 임베딩을 생성한 다음, FAISS 벡터 DB에 저장합니다.
vector_db = FAISS.from_texts(texts, embedding_model)

# 데이터베이스 상태 확인
print(f"총 {len(texts)} 개의 문서가 벡터 데이터베이스에 저장되었습니다.")

# 쿼리 문장을 임베딩하여 벡터로 변환
query = "What is a document loader?"

# 벡터 데이터베이스에서 쿼리와 유사한 문서 검색
search_results = vector_db.similarity_search(query, k=3) # 상위 3개의 유사 문서 반환

# 검색 결과 출력
for i, result in enumerate(search_results, 1):
   print(f"유사 문서 {i}:")
   print(result.page_content)
   print()
```

### 5-6. 콜백 및 평가

콜백은 랭체인 작업의 각 단계에서 발생하는 이벤트를 추적하고 모델의 성능을 모니터링하는 데 사용되는 중요한 기능이다. 이를 통해 작업 과정을 실시간으로 분석하고, 오류가 발생한 시점을 기록하여 문제를 신속히 해결할 수 있도록 돕는다.

```python
from langchain_openai import ChatOpenAI
from langchain.callbacks import get_openai_callback

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

with get_openai_callback() as cb:
   response = llm.invoke("프랑스, 영국, 스페인의 수도는?")
   print(response)
   print(f"Total Tokens: {cb.total_tokens}")
   print(f"Total Cost: {cb.total_cost}")
```
