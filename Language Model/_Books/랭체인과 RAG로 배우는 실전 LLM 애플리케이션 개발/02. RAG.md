# RAG

## 1. RAG의 배경 및 중요성

RAG의 등장은 LLM의 환각 문제를 해결하기 위한 노력에서 비롯되었다. LLM은 거대한 양의 텍스트 데이터로 학습되었지만, 때떄로 사실과 다른 정보를 생성하거나 없는 정보를 만들어내는 문제가 있었다. 특히 최신 정보나 특정 도메인의 전문 지식을 요구하는 상황에서 존재하지 않는 정보를 생성하여 AI 서비스의 신뢰도레 타격을 주었다.

RAG는 LLM의 기본 지식 베이스를 크게 확장한다. 외부 데이터 소스를 활용함으로써 모델은 훨씬 더 광범위하고 깊이 있는 정보에 접근할 수 있다. 특히 특정 도메인이나 최신 정보에 대한 쿼리에서 큰 장점을 발휘한다.

 - RAG 시스템은 생성된 응답의 근거가 되는 정보 소스를 명확히 제시할 수 있다. (AI 블랙박스 문제 해결) 
 - 외부 데이터 소스를 활용하여 비용 효율적이다.
    - 정보 업데이트가 필요할 때 전체 모델 재학습할 필요 없이 외부 데이터베이스만 업데이트하면 된다.
    - 다양한 용도에 맞게 LLM을 재사용할 수 있다.

## 2. 랭체인을 이용한 RAG

### 2-1. 환경 설정

 - OpenAIEmbeddings: 텍스트를 벡터로 변환
 - OpenAI: OpenAI의 기본 언어 모델
 - ChatOpenAI: OpenAI의 채팅 모델
 - Chroma: 벡터 데이터베이스 (임베딩된 텍스트 저장, 검색)
 - CharacterTextSplitter: 긴 텍스트를 작은 청크로 분리
 - create_retrieval_chain: 검색 기반 질의 응답 체인 생성 (문서 검색과 질문 답변을 연결하는 체인)
 - create_stuff_documents_chain: 문서 처리를 위한 체인 생성. 여러 문서를 하나의 컨텍스트로 결합하여 LLM에 전달하는 간단한 문서 처리 체인 생성
 - PromptTemplate, ChatPromptTemplate: 프롬프트 템플릿
```python
import os
from langchain_oepnai import OpenAIEmbeddings
from langchain_oepnai import OpenAI
from langchain_oepnai import ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate

os.environ['OPENAI_API_KEY']="API KEY"
```

### 2-2. 데이터 로드

RAG 시스템을 통해 가져올 비정형 데이터 정의

```python
from google.colab import drivefrom langchain_community.document_loaders import PyPDFLoader
drive.mount("/content/drive") # 드라이브 마운트

file_path = ("/content/drive/MyDrive/파일명.pdf")

loader = PyPDFLoader(file_path)
pages = []
async for page in loader.alazy_load():
    pages.append(page)
```

### 2-3. 데이터 청킹

RAG 시스템에서 문서를 효과적으로 처리하기 위해 긴 텍스트를 적절한 크기로 나누는 청킹 과정이 필수적이다. 청킹은 문서를 의미 있는 단위로 분할하여 검색과 이해를 용이하게 만드는 과정이다.

 - 오버랩 전략으로 문맥 유지를 개선하여 LLM 임베딩 검색의 정확도를 높이거나 RAG를 통해 얻은 관련 청크를 프롬프트 주입할 때보다 더 나은 결과를 얻을 수 있게한다.

```python
text_splitter = CharacterTextSplitter(
    chunk_size=500, # 각 청크의 최대 길이를 500자로 설정
    chunk_overlap=40, # 최소한의 중복만 허용하여 문맥의 연속성 유지
    length_function=len, # 텍스트 길이를 측정하는 함수 (파이썬 기본 함수 len 사용)
    separator="\n" # 줄바꿈을 기준으로 텍스트 분할
)
texts = text_splitter.split_documents(pages)
```

 - ``
```python

print("한 문장의 길이 :", len("첫 번째 문단입니다.")) # --11자

text = """첫 번째 문단입니다.
두 번째 문단입니다.
세 번째 문단입니다.
네 번째 문단입니다.
"""

# 작은 오버랩
text_splitter_small = CharacterTextSplitter(
    chunk_size=30,
    chunk_overlap=5,  # 작은 오버랩
    separator="\n"
)

print("=== 작은 오버랩 (chunk_overlap=5) ===")
chunks = text_splitter_small.split_text(text)
for i, chunk in enumerate(chunks, 1):
    print(f"청크 {i}: {chunk}\n")

"""
=== 작은 오버랩 (chunk_overlap=5) ===
청크 1: 첫 번째 문단입니다.
두 번째 문단입니다.

청크 2: 세 번째 문단입니다.
네 번째 문단입니다.
"""

# 큰 오버랩
text_splitter_large = CharacterTextSplitter(
    chunk_size=30,
    chunk_overlap=12,  # 큰 오버랩
    separator="\n"
)

print("=== 큰 오버랩 (chunk_overlap=12) ===")
chunks = text_splitter_large.split_text(text)
for i, chunk in enumerate(chunks, 1):
    print(f"청크 {i}: {chunk}\n")

"""
=== 큰 오버랩 (chunk_overlap=12) ===
청크 1: 첫 번째 문단입니다.
두 번째 문단입니다.

청크 2: 두 번째 문단입니다.
세 번째 문단입니다.

청크 3: 세 번째 문단입니다.
네 번째 문단입니다.
"""


text = """첫 번째 문단입니다.
두 번째 문단입니다.
세 번째 문단입니다.
네 번째 문단입니다."""

text_splitter = CharacterTextSplitter(
    chunk_size=50,
    chunk_overlap=15,
    separator="\n"
)

chunks = text_splitter.split_text(text)
for i, chunk in enumerate(chunks, 1):
    print(f"청크 {i}: {chunk}\n")

""" 결과
청크 1: 첫 번째 문단입니다.
두 번째 문단입니다.
세 번째 문단입니다.
네 번째 문단입니다.
"""
```

### 2-4. 벡터 스토어

텍스트 청킹이 완료되었다면 각 청크를 벡터로 변환하고 저장해야 한다. 벡터 스토어 Chroma는 이러한 벡터화된 텍스트를 효율적으로 저장하고 검색할 수 있게 해주는 데이터베이스이다.

```python
# OpenAI 임베딩 초기화
# OpenAI의 text-embedding-ada-002 모델을 사용하여 텍스트를 벡터로 변환하는 객체를 초기화합니다.
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

# Chroma 벡터 DB 생성
# texts의 각 청크를 임베딩하여 Chroma DB에 저장합니다.
vectordb = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory="chroma_db_v1"  # 벡터 DB 저장 경로
)

# 벡터 DB 저장
vectordb.persist()
```

 - `벡터 DB 사용`
```python
# 디스크에서 벡터 스토어 로드
# 벡터 데이터베이스를 chroma_db_v1 경로에서 로드하고, OpenAI 임베딩 함수를 지정하여 Chroma 객체 초기화
vectordb = Chroma(
    persist_directory="chroma_db_v1",
    embedding_function=embeddings
)

# 저장된 데이터 조회 및 출력
print("=== Chroma 벡터 DB에 저장된 데이터 ===")
collection = vectordb._collection
data = collection.get(include=['embeddings', 'documents', 'metadatas'])
print(f"\n1. 컬렉션 크기: {collection.count()} 문서")

# 첫 번째 문서의 임베딩 정보 출력
print("\n2. 첫 번째 문서의 임베딩 정보:")
print(f"벡터 차원: {len(data['embeddings'][0])}")
print(f"임베딩 벡터 (앞부분 5개 요소): {data['embeddings'][0][:5]}")

# 문서와 메타데이터 출력 (2개 예시)
print("\n3. 문서 및 메타데이터 (2개):")
for idx, (document, metadata) in enumerate(zip(data['documents'][20:22], data['metadatas'][20:22])):
    print(f"인덱스: {idx}, 문서: {document}, 메타데이터: {metadata}")
```

### 2-5. 리트리버 및 프롬프트

리트리버는 사용자의 질문과 가장 관련성 높은 문서들을 효율적으로 찾아내는 역할을 한다. 그리고 리트리버를 통해 가져온 문서들을 정제하고 언어 모델이 더 잘 이해할 수 있도록 하는 프롬프트 템플릿을 정의한다. 이때 프롬프트에는 챗봇이 따라야 할 규칙 및 답변 형태 등을 간결하게 작성해 할루시네이션을 방지한다.

```python
# MMR(Maximal Marginal Relevance) 검색 방식을 사용하여 유사도와 다양성을 모두 고려
retriever = vectordb.as_retriever(
    search_type="mmr",  # Maximal Marginal Relevance 검색
    search_kwargs={
        "k": 5,  # 최종 검색 문서 수
        "fetch_k": 8,  # 초기 검색 문서 수
        "lambda_mult": 0.7  # 다양성 가중치 (1에 가까울수록 다양성)
    }
)

# 프롬프트 작성
prompt = ChatPromptTemplate.from_messages([
    ("system", """주어진 문서들을 기반으로 질문에 정확하게 답변해주세요.
    다음 지침을 반드시 따라주세요:
    1. 문서의 정보만을 사용하여 답변하세요
    2. 제품명이 언급된 경우 반드시 포함해서 답변하세요
    3. 제품의 기능과 특징을 구체적으로 설명하세요
    4. 답변에 확신이 없는 경우, 그 부분을 명시적으로 언급하세요

    문맥: {context}"""),
    ("human", "{input}")
])

# GPT-4o-mini 모델을 사용
llm = ChatOpenAI(
    temperature=0,  # 출력이 일정하도록 온도 설정
    max_tokens=400,  # 최대 토큰 수
    model_name="gpt-4o-mini"  # 사용할 모델 이름
)

# 단일 문서 처리 체인 생성
# 여러 문서를 처리하여 답변을 생성하는 체인 생성
document_chain = create_stuff_documents_chain(
    llm=llm,
    prompt=prompt
)

# 검색 및 응답 생성을 위한 최종 체인 생성
# create_retrieval_chain 함수를 사용하여 검색기(retriever)와 문서 처리 체인(document_chain) 연결
# retriever가 질문과 관련된 문서들을 검색한 뒤, document_chain이 검색된 문서들을 처리하여 최종 답변 생성
qa_chain = create_retrieval_chain(
    retriever=retriever,
    combine_docs_chain=document_chain
)

# 최종 결과
def ask_question(question: str, qa_chain):
    # qa_chain.invoke()는 새로운 형식의 입력을 사용
    result = qa_chain.invoke({
        "input": question  # 'query' 대신 'input' 사용
    })
    print("질문:", question)
    print("\n답변:", result['answer'])  # 'result' 대신 'answer' 키 사용
    # source_documents가 있는 경우에만 출력
    if 'context' in result:
        print("\n참고 문서:")
        documents = result['context']
        for i, doc in enumerate(documents, 1):
            print(f"\n문서 {i}:")
            print(doc.page_content[:500], "...")
            if hasattr(doc, 'metadata'):
                print(f"(페이지: {doc.metadata.get('page', 'Unknown')})")
# 예시 질문
questions = [" 노인들이 일어나도록 도와주는 로봇의 제품 이름은 뭔가요? 그리고 특징을 알려주세요 "]

for question in questions:
    ask_question(question,qa_chain)
    print("\n" + "="*50 + "\n")
```

## 3. Advanced RAG

 - 정보 검색의 품질을 향상시키기 위해 사용하는 다양한 기술
    - 검색된 문서의 순서를 변경하는 리랭커
    - 가상의 문서를 생성하는 HyDE
    - 더 많은 정보를 포함하도록 쿼리를 변경하는 쿼리 확장
    - 다양한 관점을 반영하도록 다중으로 쿼리를 생성하는 멀티 쿼리

### 3-1. 문서의 순서를 조절하여 성능을 올리는 리랭커 기술

리랭커는 검색된 문서의 순위를 재조정하여 질문에 더욱 적합한 결과를 상위에 위치시키는 역항르 수행한다.

문서의 순위를 조절하는 이유는 컨텍스트로 제공되는 참고 문서가 처음과 끝에 위치할 경우 성능이 올라가고, 중간에 위치할 경우에는 성능이 크게 저하되며, 심지어 문서를 참고하지 않는 것보다 성능이 떨어질 수 있기 때문이다.

 - 문서 순위를 조절하는 전통적인 정보 검색 방법으로는 TF-IDF와 BM25가 있다.
    - TF-IDF는 문서 내 특정 단어의 빈도와 그 단어가 전체 문서에서 얼마나 일반적인지 또는 희귀한지를 평가하여 단어의 가중치를 계싼하는 기법이다.
    - TF-IDF는 문서와 쿼리 간의 단어 일치만을 기반으로 평가하므로, 단어의 의미적 유사성을 고려하지 못한다는 한계가 있다. 예를 들어, "자동차"와 "차량"은 의미가 비슷하지만, TF-IDF는 이를 다른 단어로 처리하여 연관성을 잘 반영하지 못할 수 있다.
    - BM25는 TF-IDF를 개선한 정보 검색 모델로, 문서와 쿼리 간의 관련성을 평가하는 데 있어 보다 세밀한 조정을 가능하게 한다.
    - BM25에서는 단순한 TF 대신 포화 함수를 사용하여 TF를 보정한다. 이 방식은 특정 단어의 빈도가 계속 증가해도 그 효과가 점진적으로 감소하도록 하여, 하나의 문서에 특정 단어가 너무 많이 등장할 떄의 과대 평가를 방지한다.
    - BM25의 장점으로는 문서의 길이에 따라 가중치를 조정하는 기능이 있어 짧은 문서와 긴 문서 간의 공정한 비교가 가능하다는 것이다. 같은 단어가 긴 문서에 많이 나오는 경우, 그 단어의 가중치는 덜 중요하게 반영된다.
 - 신경망 기반의 리랭커 모델은 주로 트랜스포머 기반의 모델들을 사용한다.
    - 대표적인 모델로는 BERT 리랭커가 있으며, BERT는 양방향 인코더로 쿼리와 문서의 컨텍스트를 깊게 이해하여 관련성을 평가한다.
 - 리랭커 모델은 크게 Cross-encoder 방식과 Bi-encoder 방식이 있다.
    - Cross-encoder 방식은 쿼리와 문서를 한 쌍으로 결합하여 입력으로 넣고, 두 텍스트가 얼마나 관련이 있는지에 대한 점수를 직접 계산하여 각 문서의 최종 순위를 재조정하는 방식이다.
    - Bi-encoder는 쿼리와 문서를 독립적으로 처리하여 임베딩을 계산하는 방식이다. 문서에 대한 임베딩을 오프라인에서 미리 계산할 수 있다는 장점이 있다. 쿼리에 대해서만 실시간으로 임베딩을 계산하고 코사인 유사도를 비교하면 된다.
    - Bi-encoder는 쿼리와 문서를 독립적으로 임베딩하므로, 두 입력 간의 복잡한 상호작용을 잘 반영하지 못한다는 한계가 있다.
    - 일반적으로 Cross-encoder는 Bi-encoder보다 더 높은 정확도를 제공하지만, 계싼 비용이 훨씬 많이 드는 반면, Bi-encoder는 효율성을 극대화하기 위해 설계된 모델로 빠른 검색이 필요한 경우 유리하다.

통상적인 경우 RAG 구현 시 TF-IDF의 단점을 보완한 BM25를 초기 검색 단계에서 사용하여 효율적으로 문서를 검색한 후, 리랭커 모델로 이를 재평가하여 최종 순위를 매기는 방식으로 높은 정확도를 추구할 수 있다.

```python
### 1. 리랭커 모델 설정
import torch
from langchain.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoModelForSequenceClassification, AutoTokenizer

loader = PyMuPDFLoader("/content/drive/MyDrive/Cobal Notebooks/pdf/파일명.pdf")
documents = loader.load()

# 텍스트 분할
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)

# Document 객체에서 텍스트 내용 추출
text_contents = [doc.page_content for doc in texts]

# 임베딩 모델 설정
embeddings = HuggingFaceEmbeddings(model_name="sentence=transformers/paraphrase-multilingual-MiniLM-L12-v2")

# 벡터 스토어 설정
vectorstore = FAISS.from_texts(text_contents, embeddings)

# BGE Reranker 모델 설정
model_name = "BAAI/bge-reranker-v2-m3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

### 2. 리랭킹 수행
def rerank_documents(query, docs, top_k=5):
    pairs = [[query, doc.page_content] for doc in docs]

with torch.no_grad():
    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors="pt", max_length=512)
    scores = model(**inputs).logits.squeeze(-1)
    ranked_indices = scores.argsort(descending=True)
    reranked_docs = [(docs[i], scores[i].item()) for i in ranked_indices[:top_k]]
    return reranked_docs

# 리트리버 설정
base_retriever = vectorstore.as_retriever(search_kwargs={"k":20})

# 쿼리 실행
query = "국내 고령화 전망에 대해 알려주세요."

# 기본 검색 결과 출력
print("## 기본 검색 결과")
base_docs = base_retriever.invoke(query)
for i, doc in enumerate(base_docs[:5]):
    print(f"문서 {i+1}:")
    print(doc.page_content[:100] + "...\n")

# 리랭킹 결과 출력
prunt("\n## 리랭킹 결과")
reranked_docs = rerank_documents(query, base_docs)
for i, (doc, score) in enumerate(reranked_docs):
    print(f"문서 {i+1} (점수: {score:.4f}):")
    print(doc.page_content[:100] + "...\n")
```
