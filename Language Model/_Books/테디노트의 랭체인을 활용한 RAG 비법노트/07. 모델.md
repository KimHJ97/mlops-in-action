# 모델

## 1. 다양한 LLM 모델 활용

### 1-1. OpenAI 모델

 - `temperature`: 샘플링 온도를 설정하는 옵션입니다. 값은 0과 2 사이에서 선택할 수 있습니다. 높은 값(예: 0.8)은 출력을 더 무작위하게 만들고, 낮은 값(예: 0.2)은 출력을 더 집중되고 결정론적으로 만듭니다.
 - `max_tokens`: 채팅 완성에서 생성할 토큰의 최대 개수를 지정합니다. 이 옵션은 모델이 한 번에 생성할 수 있는 텍스트의 길이를 제어합니다.
 - `model_name`: 적용 가능한 모델을 선택하는 옵션입니다. 더 자세한 정보는 OpenAI 모델 문서에서 확인할 수 있습니다.
```python
from langchain_openai import ChatOpenAI

# ChatOpenAI 객체를 생성합니다.
gpt = ChatOpenAI(
    temperature=0,
    model_name="gpt-4o",  # 모델명
)

# 스트리밍 출력을 위하여 invoke() 대신 stream()을 사용합니다.
answer = gpt.stream("사랑이 뭔가요?")

# 답변 출력
stream_response(answer)
```
<br/>

### 1-2. Claude 모델

 - `API 키 발급`: https://console.anthropic.com/settings/keys
 - `모델 리스트`: https://docs.anthropic.com/en/docs/about-claude/models

```python
from langchain_anthropic import ChatAnthropic

anthropic = ChatAnthropic(model_name="claude-3-5-sonnet-20241022")
answer = anthropic.stream("사랑이 뭔가요?")
stream_response(answer)
```
<br/>

### 1-3. Deepseek 모델

 - `API 키 발급`: https://platform.deepseek.com/api_keys
 - `모델 리스트`: https://api-docs.deepseek.com/quick_start/pricing
```python
from langchain_deepseek import ChatDeepSeek

deekseek = ChatDeepSeek(model_name="deepseek-chat")
answer = deekseek.stream("사랑이 뭔가요?")
stream_response(answer)
```
<br/>

### 1-4. Cohere 모델

```python
from langchain_cohere import ChatCohere

cohere = ChatCohere(temperature=0)
answer = cohere.stream("사랑이 뭔가요?")
stream_response(answer)
```
<br/>

### 1-5. Upstage 모델

```python
import os
from langchain_upstage import ChatUpstage
os.environ["UPSTAGE_API_KEY"] = ""
upstage = ChatUpstage(temperature=0)
answer = upstage.stream("사랑이 뭔가요?")
stream_response(answer)
```
<br/>

### 1-6. Perplexity

```python
import os
from langchain_perplexity import ChatPerplexity
os.environ["PPLX_API_KEY"] = "이곳에 API 키를 입력하세요."
pplx = ChatPerplexity(temperature=0)
answer = pplx.stream("사랑이 뭔가요?")
stream_response(answer)
```
<br/>

## 2. 캐싱

LangChain은 LLM을 위한 선택적 캐싱 레이어를 제공한다.

 - 동일한 완료를 여러 번 요청하는 경우 LLM 공급자에 대한 API 호출 횟수를 줄여 비용을 절감할 수 있습니다.
 - LLM 제공업체에 대한 API 호출 횟수를 줄여 애플리케이션의 속도를 높일 수 있습니다.

### 2-1. 인메모리 캐시

인메모리 캐시는 메모리 공간을 활용해 동일한 질문에 대한 답변을 일시적으로 저장한다. 동일한 요청이 반복될 경우 캐시된 응답을 즉시 반환한다. 인메모리 캐시는 프로그램 종료시 메모리가 초기화되면서 캐시가 모두 사라지기 떄문에 프로그램이 재시작되면 캐시를 다시 만들어야 한다.

 - set_llm_cache()가 전역(Global) 수준에서 모든 LLM 호출을 가로채 자동으로 LLM 응답을 캐싱한다.
 - 같은 입력일 때만 캐시가 동작한다. "한국"과 "대한민국"은 다르다.
 - `Runnable` 체인에서 캐시를 명시적으로 끈 경우 (`configurable={"cache": False}`) 캐싱 하지 않는다.
```python
%%time
from langchain.globals import set_llm_cache
from langchain.cache import InMemoryCache

# 인메모리 캐시를 사용합니다.
set_llm_cache(InMemoryCache())

# 체인을 실행합니다.
response = chain.invoke({"country": "한국"})
print(response.content)
```
<br/>

### 2-2. SQLite 캐시

SQLite 캐시는 데이터베이스 파일을 활용해 캐시 데이터를 저장하여 프로그램을 종료했다가 다시 시작해도 캐시 정보를 유지할 수 있다. 동일한 질문에 대한 답변을 장기적으로 캐싱할 수 있어 재시작 후에도 캐싱된 답변을 빠르게 반환할 수 있다.

```python
from langchain_community.cache import SQLiteCache
from langchain_core.globals import set_llm_cache
import os

# 캐시 디렉토리를 생성합니다.
if not os.path.exists("cache"):
    os.makedirs("cache")

# SQLiteCache를 사용합니다.
set_llm_cache(SQLiteCache(database_path="cache/llm_cache.db"))

# 체인을 실행합니다.
response = chain.invoke({"country": "한국"})
print(response.content)
```
<br/>

## 3. 모델 직렬화 (저장 및 로드)

직렬화는 데이터 구조나 객체의 상태를 저장하거나 전송하기 위해 일련의 바이트나 문자열 형식으로 변환하는 과정이다.

역직렬화는 직렬화된 데이터를 원래 객체나 데이터 구조의 형태로 복원하는 과정이다.

학습/추론 파이프라인을 저장하고, 다른 환경(서버, 배포 등)에서 그대로 재사용할 수 있다.

### 3-1. API

LangChain의 객체들은 대부분 Runnable 인터페이스를 구현하고 있어서, 이를 쉽게 직렬화/역직렬화할 수 있다.

 - `dumps(obj)`: 객체를 JSON 문자열로 직렬화
 - `loads(json_str)`: JSON 문자열을 객체로 복원
 - `dump(obj, file_path)`: 파일로 직렬화
 - `load(file_path)`: 파일에서 역직렬화
```python
from langchain.load import dumps, loads, dump, load
```

<br/>

### 3-2. 간단한 LLMChain 저장 & 불러오기

 - `LangChain <0.1.0` 또는 `LegacyChain` 사용 시 Pickle을 이용하여 JSON 문자열을 파일로 저장/로드
```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.load import dumps, loads, dump, load

# 1️⃣ 체인 정의
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
prompt = PromptTemplate.from_template("대한민국의 수도는 어디인가요?")
chain = LLMChain(llm=llm, prompt=prompt)

# 2️⃣ JSON 문자열로 직렬화
chain_json = dumps(chain)
print(chain_json[:200])  # 일부만 출력

# 3️⃣ 파일로 저장
dump(chain, "chain.json")

# 4️⃣ 파일에서 다시 로드
# chain.save("chain.json")
loaded_chain = load("chain.json")

# 5️⃣ 실행해보기
result = loaded_chain.invoke({})
print(result)
```
<br/>

## 4. 토큰 사용량 확인

### 4-1. 기본 응답 메타데이터

LangChain은 OpenAI API에서 반환된 usage 정보(prompt_tokens, completion_tokens, total_tokens)를 자동으로 저장한다.

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
response = llm.invoke("한국의 수도는 어디인가요?")
print(response.content)
print(response.response_metadata)

'''
서울
{'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18},
 'model_name': 'gpt-4o-mini',
 'system_fingerprint': 'fp_xxxxxx'}
'''
```
<br/>

### 4-2. 콜백 핸들러

 - 체인을 사용하면 내부적으로 LLM이 여러 번 호출될 수 있다. LangChain에서는 callback(콜백 핸들러)을 이용해 전체 토큰 사용량을 집계할 수 있다.
 - cb.total_tokens, cb.total_cost 등으로 접근 가능
```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.callbacks import get_openai_callback

prompt = PromptTemplate.from_template("서울은 어느 나라의 수도인가요?")
llm = ChatOpenAI(model="gpt-4o-mini")
chain = LLMChain(prompt=prompt, llm=llm)

# 콜백으로 사용량 추적
with get_openai_callback() as cb:
    result = chain.invoke({})
    print(result)
    print(cb)  # 사용량 요약 출력

'''
서울은 대한민국의 수도입니다.
Tokens Used: 22
Prompt Tokens: 8
Completion Tokens: 14
Total Cost (USD): 0.000022
'''
```
<br/>

## 5. 구글 생성 AI

 - Google API 키를 환경 변수 GOOGLE_API_KEY로 설정
```python
import base64
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

# 1️⃣ 모델 초기화 (이미지 입력을 지원하는 모델이어야 함)
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")

# 2️⃣ 로컬 이미지 파일 읽기 → base64 인코딩
image_path = "images/sample.png"  # 로컬 이미지 경로
with open(image_path, "rb") as f:
    encoded = base64.b64encode(f.read()).decode("utf-8")

# 3️⃣ Data URI 형태로 변환
data_uri = f"data:image/png;base64,{encoded}"

# 4️⃣ HumanMessage로 텍스트 + 이미지 전달
message = HumanMessage(
    content=[
        {"type": "text", "text": "이 사진의 내용을 간단히 설명해 주세요."},
        {"type": "image_url", "image_url": data_uri},
    ]
)

# 5️⃣ 모델 호출
response = llm.invoke([message])

# 6️⃣ 결과 출력
print(response.content)
```
<br/>

## 6. 허깅페이스 엔드포인트

허깅페이스는 무료 모델과 데이터셋을 공유할 수 있는 오픈소스 플랫폼으로 사용자는 다양한 머신러닝 애플리케이션을 구축하고 배포할 수 있으며, 이를 위한 엔드포인트 서비스도 제공된다. 엔드포인트란 사용자가 원격에서 머신러닝 모델을 호출하고 사용할 수 있도록 설정된 URL 또는 API 접근 지점을 의미한다.

허깅페이스의 서버리스 엔드포인트와 Inference API는 이러한 원격 호출과 배포를 간소화 해주는 서비스이다. 서버리스 엔드포인트는 사용자가 직접 서버를 관리할 필요 없이 모델을 호스팅하고 자동확장할 수 있는 API로, 인프라 설정 및 유지보수는 허깅페이스가 알아서 처리한다. `Inference API`는 허깅페이스에 호스팅된 모델을 즉시 호출할 수 있는 간편한 추론 API이다. 소규모 추론 요청이나 모델 테스트에 적합하며, 복잡한 설정 없이 HTTP 요청을 통해 다양한 사전 학습 모델을 바로 사용할 수 있다.

 - 토큰 발급 주소: https://huggingface.co/docs/hub/security-tokens
    - HUGGINGFACEHUB_API_TOKEN
 - 참고 모델
    - 허깅페이스 LLM 리더보드: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
    - 모델 리스트: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads
    - LogicKor 리더보드: https://lk.instruct.kr/

```python
!pip install -qU huggingface_hub

from dotenv import load_dotenv
from huggingface_hub import login

load_dotenv()
login("API 토큰")
```

### 6-1. 허깅페이스 엔드포인트

Hugging Face의 모델을 서버에 직접 올리지 않고, 클라우드에서 API 형태로 바로 호출할 수 있게 해주는 기능이다.

 - `Serverless Endpoints`
```python
from langchain.prompts import PromptTemplate
import os
from langchain_core.output_parsers import StrOutputParser
from langchain_huggingface import HuggingFaceEndpoint

# 프롬프트 정의
template = """<|system|>
You are a helpful assistant.<|end|>
<|user|>
{question}<|end|>
<|assistant|>"""
prompt = PromptTemplate.from_template(template)

# 모델 정의
repo_id = "microsoft/Phi-3-mini-4k-instruct"
llm = HuggingFaceEndpoint(
    repo_id=repo_id,  # 모델 저장소 ID를 지정합니다.
    max_new_tokens=256,  # 생성할 최대 토큰 길이를 설정합니다.
    temperature=0.1,
    huggingfacehub_api_token=os.environ["HUGGINGFACEHUB_API_TOKEN"],  # 허깅페이스 토큰
)

# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.
chain = prompt | llm | StrOutputParser()
# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.
response = chain.invoke({"question": "what is the capital of South Korea?"})
print(response)
```
<br/>

 - `전용 엔드포인트(Dedicated Endpoint)`
    - 무료 서버리스 API를 사용하면 솔루션을 빠르게 구현하고 반복할 수 있습니다. 하지만 로드가 다른 요청과 공유되기 때문에 대용량 사용 사례에서는 속도 제한이 있을 수 있습니다.
    - 엔터프라이즈 워크로드의 경우, Inference Endpoints - Dedicated를 사용하는 것이 가장 좋습니다. 이를 통해 더 많은 유연성과 속도를 제공하는 완전 관리형 인프라에 액세스할 수 있습니다.
    - https://huggingface.co/inference-endpoints/dedicated
```python
from langchain.llms import HuggingFaceEndpoint

llm = HuggingFaceEndpoint(
    endpoint_url="https://your-endpoint.hf.space",
    huggingfacehub_api_token="hf_xxx",
    max_new_tokens=512,
    temperature=0.01,
)
result = llm.invoke("Write a haiku about the ocean.")
print(result)
```
<br/>

## 7. 허깅페이스 로컬 파이프라인(HuggingFace Local Pipelines)

HuggingFace Local Pipelines는 Hugging Face의 파이프라인(Pipeline) API를 로컬 환경에서 실행하는 방식을 말한다.

 - HuggingFacePipeline.from_model_id는 내부적으로 transformers 라이브러리의 pipeline 함수를 호출합니다.
 - pipeline은 모델과 토크나이저를 로컬 캐시에서 먼저 찾고, 없으면 자동으로 다운로드합니다.
 - 즉, 로컬에 없던 모델이면 Hugging Face Hub에서 자동으로 다운로드되고, 이후에는 로컬 캐시에서 사용됩니다.
```python
import os
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ./cache/ 경로에 다운로드 받도록 설정
os.environ["TRANSFORMERS_CACHE"] = "./cache/"
os.environ["HF_HOME"] = "./cache/"

# HuggingFace 모델을 다운로드 받습니다.
hf = HuggingFacePipeline.from_model_id(
    model_id="beomi/llama-2-ko-7b",  # 사용할 모델의 ID를 지정합니다.
    task="text-generation",  # 수행할 작업을 지정합니다. 여기서는 텍스트 생성입니다.
    device=0,  # GPU 디바이스 번호를 지정합니다. -1은 CPU를 의미합니다.
    batch_size=2,  # 배치 크기s를 조정합니다. GPU 메모리와 모델 크기에 따라 적절히 설정합니다.
    # 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.
    pipeline_kwargs={"max_new_tokens": 512},
)

# 프롬프트 정의
template = """Answer the following question in Korean.
#Question: 
{question}

#Answer: """
prompt = PromptTemplate.from_template(template)  # 템플릿을 사용하여 프롬프트 객체 생성

# 프롬프트와 언어 모델을 연결하여 체인 생성
chain = prompt | hf | StrOutputParser()
question = "대한민국의 수도는 어디야?"  # 질문 정의
chain.invoke({"question": question})
```
<br/>

## 8. Ollama

Ollama는 로컬(내 PC 또는 사내 서버)에서 대형 언어 모델(LLM, Large Language Model)을 실행하고 관리할 수 있게 해주는 오픈소스 플랫폼이자 실행 엔진이다.

 - 설치 주소: https://ollama.com/
 - 로컬 실행 지원: Windows, macOS, Linux에서 실행 가능하며 인터넷 연결 없이도 실행 가능한 모델도 존재합니다. 
 - 다양한 모델 지원: 예컨대 LLaMA 3, Mistral, Gemma3 등 다양한 공개 모델을 다운로드해서 사용할 수 있습니다.
 - 간편한 명령어 실행: 예를 들어 ollama run gemma3:27b 처럼 간단한 명령어로 모델을 실행할 수 있습니다.

<br/>

### 8-1. 사용 방법

```bash
# 모델 다운로드
ollama pull <모델명>
ollama pull gemma3:7b   # Gemma3 모델, 7B 파라미터
ollama pull llama3:13b  # LLaMA3 모델, 13B 파라미터

# 모델 실행
ollama run <모델명>
ollama run gemma3:7b

# REST API 사용
# 로컬 서버를 켜고 다른 프로그램에서 API로 모델을 호출할 수도 있다.
ollama serve

import requests
url = "http://localhost:11434/api/generate"
data = {"model": "gemma3:7b", "prompt": "안녕하세요, Ollama!"}
response = requests.post(url, json=data)
print(response.json()["completion"])
```
<br/>

### 8-2. LangChain + Ollama

```python
from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOllama(model="EEVE-Korean-10.8B:latest")
prompt = ChatPromptTemplate.from_template("{topic} 에 대하여 간략히 설명해 줘.")
chain = prompt | llm | StrOutputParser()
answer = chain.invoke({"topic": "deep learning"})
```
<br/>

### 8-3. 멀티모달(Multimodal) 지원

Ollama는 Bakllava와 Llava와 같은 멀티모달 LLM을 지원한다. ollama pull llava:7b 또는 ollama pull bakllava 명령어로 멀티모달 LLM을 다운로드할 수 있다.

멀티모달이란 여러 종류의 데이터 모드를 동시에 처리하거나 이해할 수 있는 능력을 말한다. 즉, 멀티모달 모델은 텍스트만, 혹은 이미지만 처리하는 것이 아니라, 여러 형태의 데이터를 함께 이해하고 분석할 수 있습니다. (텍스트, 이미지, 오디오, 비디오, 센서 데이터 등)

 - 이미지를 인식하는 멀티모달을 구현하려면 이미지를 base64로 인코딩된 문자열로 변환하는 코드가 필요하다.
```python
import base64
from io import BytesIO

from IPython.display import HTML, display
from PIL import Image
from langchain_core.messages import HumanMessage


def convert_to_base64(pil_image):
    """
    PIL 이미지를 Base64로 인코딩된 문자열로 변환합니다.

    :param pil_image: PIL 이미지
    :return: 크기 조정된 Base64 문자열
    """

    buffered = BytesIO()
    pil_image.save(buffered, format="JPEG")  # 필요한 경우 형식을 변경할 수 있습니다.
    img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    return img_str


def plt_img_base64(img_base64):
    """
    Base64로 인코딩된 문자열을 이미지로 표시합니다.

    :param img_base64:  Base64 문자열
    """
    # Base64 문자열을 소스로 사용하여 HTML img 태그 생성
    image_html = f'<img src="data:image/jpeg;base64,{img_base64}" />'
    # HTML을 렌더링하여 이미지 표시
    display(HTML(image_html))


def prompt_func(data):  # 프롬프트 함수를 정의합니다.
    text = data["text"]  # 데이터에서 텍스트를 가져옵니다.
    image = data["image"]  # 데이터에서 이미지를 가져옵니다.

    image_part = {  # 이미지 부분을 정의합니다.
        "type": "image_url",  # 이미지 URL 타입을 지정합니다.
        "image_url": f"data:image/jpeg;base64,{image}",  # 이미지 URL을 생성합니다.
    }

    content_parts = []  # 콘텐츠 부분을 저장할 리스트를 초기화합니다.

    text_part = {"type": "text", "text": text}  # 텍스트 부분을 정의합니다.

    content_parts.append(image_part)  # 이미지 부분을 콘텐츠 부분에 추가합니다.
    content_parts.append(text_part)  # 텍스트 부분을 콘텐츠 부분에 추가합니다.

    return [HumanMessage(content=content_parts)]  # HumanMessage 객체를 반환합니다.


file_path = "./images/jeju-beach.jpg"
pil_image = Image.open(file_path)
image_b64 = convert_to_base64(pil_image)
plt_img_base64(image_b64)
```
<br/>

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage

llm = ChatOllama(model="llava:7b", temperature=0)
chain = prompt_func | llm | StrOutputParser()
query_chain = chain.invoke(  # 체인을 호출하여 쿼리를 실행합니다.
    # 텍스트와 이미지를 전달합니다.
    {"text": "Describe a picture in bullet points", "image": image_b64}
)

print(query_chain)  # 쿼리 결과를 출력합니다.
```
<br/>

## 9. GPT4ALL

GPT4All은 개인 컴퓨터와 같은 로컬 환경에서 대규모 언어 모델(LLM)을 안전하고 쉽게 실행할 수 있도록 설계된 오픈 소스 소프트웨어입니다. 사용자가 인터넷 연결 없이도 다양한 언어 모델을 다운로드하여 실행하고 상호 작용할 수 있는 데스크톱 애플리케이션 형태로 제공됩니다.

 - 프라이버시 우선 (Privacy-First): 모든 데이터와 대화 내용이 사용자의 로컬 장치에만 저장되므로, 외부 서버로 정보가 전송되거나 유출될 위험이 없습니다. 민감한 정보를 다룰 때 높은 수준의 프라이버시를 보장합니다.
 - 로컬 실행 (Local Execution): Mac M 시리즈, AMD, NVIDIA GPU 등 다양한 일반 소비자 하드웨어에서 LLM을 실행할 수 있도록 최적화되어 있습니다. 이는 양자화(Quantization) 기술을 적용하여 모델 크기를 줄였기 때문에 가능합니다.
 - 광범위한 모델 지원: LLaMa, Mistral, Nous-Hermes, Qwen, Phi 등 1,000개 이상의 다양한 오픈 소스 언어 모델을 다운로드하여 실행할 수 있는 카탈로그를 제공합니다.
 - LocalDocs 기능: 사용자의 로컬 문서 파일(PDF, Markdown 등)을 가져와 인터넷 연결 없이 해당 문서에 대한 질문, 요약, 검색 등을 수행할 수 있는 플러그인 기능을 지원합니다 (RAG, Retrieval-Augmented Generation 기반).
 - 오픈 소스 및 커스터마이즈: MIT 라이선스를 따르는 오픈 소스 프로젝트로, 사용자가 시스템 프롬프트, 온도(Temperature), 컨텍스트 길이 등 다양한 설정을 직접 변경하여 챗봇 경험을 맞춤화할 수 있습니다.

### 9-1.

```python
# !pip install -qU gpt4all
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms import GPT4All
from langchain_core.output_parsers import StrOutputParser
from langchain_core.callbacks import StreamingStdOutCallbackHandler

# 프롬프트
prompt = ChatPromptTemplate.from_template(
    """<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>
<s>Human: {question}</s>
<s>Assistant:
"""
)

local_path = "./models/EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf"  # 원하는 로컬 파일 경로로 대체하세요.
llm = GPT4All(
    model=local_path,
    backend="gpu",  # GPU 설정
    streaming=True,  # 스트리밍 설정
    callbacks=[StreamingStdOutCallbackHandler()],  # 콜백 설정
)
chain = prompt | llm | StrOutputParser()
response = chain.invoke({"question": "대한민국의 수도는 어디인가요?"})
```
<br/>

## 10. 비디오(Video) 질의 응답 LLM (Gemini)

 - https://ai.google.dev/api/rest/v1/models/generateContent#media
 - 주요 흐름
    - File API를 사용하여 비디오 파일을 업로드합니다.
    - GenerateContent 요청을 통해 비디오에 대한 질문을 요청합니다.
    - 생성된 응답을 확인합니다.
    - 업로드한 Video 파일을 삭제합니다.
 
```python
import google.generativeai as genai
import time

# 1. File API를 사용하여 비디오 파일을 업로드한다.
# 파일 업로드 및 파일 객체 반환
print(f"파일을 업로드 중입니다...")
video_file_name = "sample-video.mp4"
video_file = genai.upload_file(path=video_file_name)
print(f"업로드 완료: {video_file.uri}")


# 2. 파일을 업로드한 후, files.get 을 호출하여 API가 파일을 성공적으로 완료되었는지 확인할 수 있따.
# 비디오 파일 처리 상태 확인
while video_file.state.name == "PROCESSING":
    # 처리 완료 대기 메시지 출력
    print("비디오 업로드 및 전처리가 완료될 때까지 잠시만 기다려주세요...")
    # 10초 대기
    time.sleep(10)
    # 비디오 파일 상태 갱신
    video_file = genai.get_file(video_file.name)

# 처리 실패 시 예외 발생
if video_file.state.name == "FAILED":
    raise ValueError(video_file.state.name)

# 처리 완료 메시지 출력
print(
    f"\n비디오 처리가 완료되었습니다!\n이제 대화를 시작할 수 있어요: " + video_file.uri
)

'''
비디오 업로드 및 전처리가 완료될 때까지 잠시만 기다려주세요...
비디오 업로드 및 전처리가 완료될 때까지 잠시만 기다려주세요...
비디오 업로드 및 전처리가 완료될 때까지 잠시만 기다려주세요...

비디오 처리가 완료되었습니다!
이제 대화를 시작할 수 있어요: https://generativelanguage.googleapis.com/v1beta/files/..
'''


# 3. 비디오가 업로드된 후, generate_content 함수를 사용하여 Video 에 대한 질문을 요청할 수 있다.
prompt = "이 영상에 대해서 짧게 요약해 줄 수 있나요?"
model = genai.GenerativeModel(model_name="models/gemini-1.5-flash")
response = model.generate_content(
    [prompt, video_file], request_options={"timeout": 600}
)
print(response.text)


# 4. 파일은 2일 후 자동으로 삭제되거나 files.delete()를 사용하여 수동으로 삭제할 수 있다.
genai.delete_file(video_file.name)
print(f"영상을 삭제했습니다: {video_file.uri}")
```
